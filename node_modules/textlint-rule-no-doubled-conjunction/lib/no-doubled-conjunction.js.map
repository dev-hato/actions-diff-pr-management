{"version":3,"sources":["../src/no-doubled-conjunction.js"],"names":["context","options","helper","RuleHelper","Syntax","report","getSource","RuleError","Paragraph","node","isChildNode","Link","Image","BlockQuote","Emphasis","source","StringSource","text","toString","isSentenceNode","type","SentenceSyntax","Sentence","sentences","SeparatorParser","separatorCharacters","filter","length","then","tokenizer","selectConjenction","sentence","tokens","tokenizeForSentence","raw","conjunctionTokens","token","pos","prev_token","map","reduce","prev","current","current_tokens","prev_sentence","prev_tokens","surface_form","conjunctionSurface","originalIndex","originalIndexFromPosition","line","loc","start","column","word_position","padding","index"],"mappings":"AAAA;AACA;;;;;;;AACA;;AACA;;AACA;;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACe,kBAAUA,OAAV,EAAiC;AAAA,MAAdC,OAAc,uEAAJ,EAAI;AAC5C,MAAMC,MAAM,GAAG,IAAIC,8BAAJ,CAAeH,OAAf,CAAf;AACA,MAAM;AAAEI,IAAAA,MAAF;AAAUC,IAAAA,MAAV;AAAkBC,IAAAA,SAAlB;AAA6BC,IAAAA;AAA7B,MAA2CP,OAAjD;AACA,SAAO;AACH,KAACI,MAAM,CAACI,SAAR,EAAmBC,IAAnB,EAAyB;AACrB,UAAIP,MAAM,CAACQ,WAAP,CAAmBD,IAAnB,EAAyB,CAACL,MAAM,CAACO,IAAR,EAAcP,MAAM,CAACQ,KAArB,EAA4BR,MAAM,CAACS,UAAnC,EAA+CT,MAAM,CAACU,QAAtD,CAAzB,CAAJ,EAA+F;AAC3F;AACH;;AACD,UAAMC,MAAM,GAAG,IAAIC,kCAAJ,CAAiBP,IAAjB,CAAf;AACA,UAAMQ,IAAI,GAAGF,MAAM,CAACG,QAAP,EAAb;;AACA,UAAMC,cAAc,GAAIV,IAAD,IAAUA,IAAI,CAACW,IAAL,KAAcC,yBAAeC,QAA9D;;AACA,UAAMC,SAAS,GAAG,6BAAeN,IAAf,EAAqB;AACnCO,QAAAA,eAAe,EAAE;AACbC,UAAAA,mBAAmB,EAAE,CACjB,GADiB,EACZ;AACL,aAFiB,EAEZ;AACL,aAHiB,EAGZ;AACL,aAJiB,EAIZ;AACL,aALiB,EAKZ;AACL,aANiB,EAMZ;AACL,aAPiB,CAOb;AAPa;AADR;AADkB,OAArB,EAYfC,MAZe,CAYRP,cAZQ,CAAlB,CAPqB,CAoBrB;AACA;;AACA,UAAII,SAAS,CAACI,MAAV,KAAqB,CAAzB,EAA4B;AACxB;AACH;;AACD,aAAO,+BAAeC,IAAf,CAAoBC,SAAS,IAAI;AACpC,YAAMC,iBAAiB,GAAIC,QAAD,IAAc;AACpC,cAAMC,MAAM,GAAGH,SAAS,CAACI,mBAAV,CAA8BF,QAAQ,CAACG,GAAvC,CAAf;AACA,cAAMC,iBAAiB,GAAGH,MAAM,CAACN,MAAP,CAAeU,KAAD,IAAWA,KAAK,CAACC,GAAN,KAAc,KAAvC,CAA1B;AACA,iBAAO,CAACN,QAAD,EAAWI,iBAAX,CAAP;AACH,SAJD;;AAKA,YAAIG,UAAU,GAAG,IAAjB;AACAf,QAAAA,SAAS,CAACgB,GAAV,CAAcT,iBAAd,EAAiCU,MAAjC,CAAwC,CAACC,IAAD,EAAOC,OAAP,KAAmB;AACvD,cAAM,CAACX,QAAD,EAAWY,cAAX,IAA6BD,OAAnC;AACA,cAAM,CAACE,aAAD,EAAgBC,WAAhB,IAA+BJ,IAArC;AACA,cAAIL,KAAK,GAAGE,UAAZ;;AACA,cAAIO,WAAW,IAAIA,WAAW,CAAClB,MAAZ,GAAqB,CAAxC,EAA2C;AACvCS,YAAAA,KAAK,GAAGS,WAAW,CAAC,CAAD,CAAnB;AACH;;AACD,cAAIF,cAAc,CAAChB,MAAf,GAAwB,CAA5B,EAA+B;AAC3B,gBAAIS,KAAK,IAAIO,cAAc,CAAC,CAAD,CAAd,CAAkBG,YAAlB,KAAmCV,KAAK,CAACU,YAAtD,EAAoE;AAChE,kBAAMC,kBAAkB,GAAGX,KAAK,CAACU,YAAjC;AACA,kBAAME,aAAa,GAAGjC,MAAM,CAACkC,yBAAP,CAAiC;AACnDC,gBAAAA,IAAI,EAAEnB,QAAQ,CAACoB,GAAT,CAAaC,KAAb,CAAmBF,IAD0B;AAEnDG,gBAAAA,MAAM,EAAEtB,QAAQ,CAACoB,GAAT,CAAaC,KAAb,CAAmBC,MAAnB,IAA6BV,cAAc,CAAC,CAAD,CAAd,CAAkBW,aAAlB,GAAkC,CAA/D;AAF2C,eAAjC,CAAtB,CAFgE,CAMhE;;AACA,kBAAMC,OAAO,GAAG;AACZC,gBAAAA,KAAK,EAAER;AADK,eAAhB;AAGA3C,cAAAA,MAAM,CAACI,IAAD,EAAO,IAAIF,SAAJ,+CAAuBwC,kBAAvB,2FAA2DQ,OAA3D,CAAP,CAAN;AACH;AACJ;;AACDjB,UAAAA,UAAU,GAAGF,KAAb;AACA,iBAAOM,OAAP;AACH,SAvBD;AAwBH,OA/BM,CAAP;AAgCH;;AA1DE,GAAP;AA4DH;;AAAA","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport { RuleHelper } from \"textlint-rule-helper\";\nimport { getTokenizer } from \"kuromojin\";\nimport { split as splitSentences, Syntax as SentenceSyntax } from \"sentence-splitter\";\nimport { StringSource } from \"textlint-util-to-string\";\n\n/*\n    1. Paragraph Node -> text\n    2. text -> sentences\n    3. tokenize sentence\n    4. report error if found word that match the rule.\n\n    TODO: need abstraction\n */\nexport default function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    const { Syntax, report, getSource, RuleError } = context;\n    return {\n        [Syntax.Paragraph](node) {\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const source = new StringSource(node);\n            const text = source.toString();\n            const isSentenceNode = (node) => node.type === SentenceSyntax.Sentence;\n            const sentences = splitSentences(text, {\n                SeparatorParser: {\n                    separatorCharacters: [\n                        \".\", // period\n                        \"．\", // (ja) zenkaku-period\n                        \"。\", // (ja) 句点\n                        \"?\", // question mark\n                        \"!\", //  exclamation mark\n                        \"？\", // (ja) zenkaku question mark\n                        \"！\" // (ja) zenkaku exclamation mark\n                    ]\n                }\n            }).filter(isSentenceNode);\n            // if not have a sentence, early return\n            // It is for avoiding error of emptyArray.reduce().\n            if (sentences.length === 0) {\n                return;\n            }\n            return getTokenizer().then(tokenizer => {\n                const selectConjenction = (sentence) => {\n                    const tokens = tokenizer.tokenizeForSentence(sentence.raw);\n                    const conjunctionTokens = tokens.filter((token) => token.pos === \"接続詞\");\n                    return [sentence, conjunctionTokens];\n                };\n                let prev_token = null;\n                sentences.map(selectConjenction).reduce((prev, current) => {\n                    const [sentence, current_tokens] = current;\n                    const [prev_sentence, prev_tokens] = prev;\n                    let token = prev_token;\n                    if (prev_tokens && prev_tokens.length > 0) {\n                        token = prev_tokens[0];\n                    }\n                    if (current_tokens.length > 0) {\n                        if (token && current_tokens[0].surface_form === token.surface_form) {\n                            const conjunctionSurface = token.surface_form;\n                            const originalIndex = source.originalIndexFromPosition({\n                                line: sentence.loc.start.line,\n                                column: sentence.loc.start.column + (current_tokens[0].word_position - 1)\n                            });\n                            // padding position\n                            const padding = {\n                                index: originalIndex\n                            };\n                            report(node, new RuleError(`同じ接続詞（${conjunctionSurface}）が連続して使われています。`, padding));\n                        }\n                    }\n                    prev_token = token;\n                    return current;\n                });\n            });\n        }\n    }\n};\n"],"file":"no-doubled-conjunction.js"}